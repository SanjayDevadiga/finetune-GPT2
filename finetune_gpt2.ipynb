{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ku6Jtqs7myRc"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "NP-Cc3fcm3-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset as a single Dataset (not DatasetDict)\n",
        "dataset = load_dataset(\"json\", data_files=\"datasets/dataset.jsonl\", split=\"train\")\n",
        "\n",
        "# Now split manually using the 'split' column inside your .jsonl data\n",
        "train_dataset = dataset.filter(lambda x: x['split'] == 'train')\n",
        "test_dataset = dataset.filter(lambda x: x['split'] == 'test')"
      ],
      "metadata": {
        "id": "Uid9nNhSstZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('gpt2').to(device)\n",
        "\n",
        "# Set the EOS token as the padding token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Tokenize the dataset\n",
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "    inputs['labels'] = inputs['input_ids'].copy()\n",
        "    return inputs\n",
        "\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "GszJU6MqnmDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='results/',\n",
        "    num_train_epochs=50,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='Logs/'\n",
        ")\n",
        "\n",
        "# Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        ")"
      ],
      "metadata": {
        "id": "d_k1FF8pnq5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "y4O6Xebnnu_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model and tokenizer explicitly\n",
        "model_output_dir = 'models/testGpt2'\n",
        "\n",
        "model.save_pretrained(model_output_dir)\n",
        "tokenizer.save_pretrained(model_output_dir)"
      ],
      "metadata": {
        "id": "JkhTnCR6rJGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_model = pipeline(\"text-generation\", model=\"models/testGpt2\", tokenizer=\"gpt2\")\n",
        "\n",
        "prompt = \"Question: Who built the Vidhana Souda?\\nAnswer:\"\n",
        "result = qa_model(prompt, max_new_tokens=50)\n",
        "print(result[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "V9GPQJJ20atw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}